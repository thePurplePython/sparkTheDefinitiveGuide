{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: The Definitive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4: Production Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "online = '/Users/grp/sparkTheDefinitiveGuide/data/retail-data/all/online-retail-dataset.csv'\n",
    "flights2105 = '/Users/grp/sparkTheDefinitiveGuide/data/flight-data/csv/2015-summary.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #15 - How Spark Runs on a Cluster_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Application Architecture:\n",
    "-  Spark Driver:\n",
    "    -  process controling execution of Spark Application\n",
    "    -  requests resources from cluster manager\n",
    "    -  maintains state of the application running on cluster\n",
    "-  Spark Executors:\n",
    "    -  processes that perform the _tasks_ assigned by Spark Driver\n",
    "    -  reports back _task_ state (success or failure) to Spark Driver\n",
    "-  Cluster Manager:\n",
    "    -  manages physical cluster of machines running Spark Application\n",
    "    -  contains resources that Spark Application requests\n",
    "    -  YARN, Mesos, Spark Standalone\n",
    "    -  _edge (gateway) node_ are machines not co-located on the cluster\n",
    "    -  Spark Driver process exists in Application Master (\"cluster driver\")\n",
    "-  SparkSession:\n",
    "    -  entry point to programming Spark\n",
    "    -  combines separate contexts [SparkContext and SQLContext] from Spark 1.x\n",
    "\n",
    "### Execution Modes:\n",
    "-  Cluster mode:\n",
    "    -  submit scripts (.jar, .py, .r) to cluster manager\n",
    "    -  CM launches driver process on worker node in cluster as well as executor processes across worker nodes\n",
    "-  Client mode:\n",
    "    -  Spark driver remains on the client machine that submitted the application\n",
    "    -  client machine is responsbile for maintaining the Spark driver process; cluster manager maintains the executor processes\n",
    "-  Local mode:\n",
    "    -  runs entire Spark Application on a single machine while achieving parallelism through threads on that single machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Life Cycle of a _Spark Application_ (Outside Spark => Infrastructure):\n",
    "1.  submit application (pre-compiled jar or library)\n",
    "2.  application makes request to _CM Driver_ node asking for resources for _Spark Driver Process_\n",
    "3.  _Spark Driver Process_ is placed on node within the cluster\n",
    "4.  code starts running and _SparkSession_ initializes a _Spark Cluster_ [driver + executors] / communicates with CM to orchestrate _Spark Executor Processes_\n",
    "5.  CM launches _Spark Executor Processes_ and returns information back to the _Spark Driver Process_\n",
    "6.  _Spark Cluster_ is now in session\n",
    "7.  _Spark Application_ is running with _Spark Driver_ scheduling tasks onto each worker and each worker responds back to _Spark Driver_ with status of tasks assigned (success or failure)\n",
    "8.  _CM_ shuts down _Spark Executors_ within _Spark Cluster_ for _Spark Driver_ when application has completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Life Cycle of a _Spark Application_ (Inside Spark => Internal Code Process):\n",
    "-  all Spark code compiles down to RDDs\n",
    "\n",
    "    1.  create _SparkSession_ for _Spark Application_\n",
    "    2.  compile transformation(s)\n",
    "    3.  trigger action(s)\n",
    "\n",
    "#### Spark Job(s):\n",
    "-  each application is made up of 1 or more _Spark Jobs_\n",
    "-  _Spark Jobs_ are executed serially\n",
    "-  an _Action_ is an execution of a _Spark Job_ [**individually consists of _Spark Stages_ and _Spark Tasks_**]\n",
    "-  mostly always 1 _Spark Job_ for 1 _Spark Action_\n",
    "\n",
    "#### Spark Stage(s):\n",
    "-  each _Spark Job_ breaks down into a series of _Spark Stages_ [**# of stages depends on how many shuffle operations need to take place**]\n",
    "\n",
    "#### Spark Task(s):\n",
    "-  _Spark Stages_ contain groups of _Spark Tasks_ [**compute operations on multiple machines**]:\n",
    "-  engine starts new _Spark Stages_ after operations called _Spark Shuffles_ [**physical re-partitioning of data**]:\n",
    "-  each _Spark Task_ corresponds to a **combination of blocks of data and a set of transformations that will run on a single executor**\n",
    "-  number of partitions = number of tasks (ex: 1,000 small partitions means 1,000 tasks executed in parallel)\n",
    "-  the more partitions means the more parallelism\n",
    "\n",
    "#### Pipelining:\n",
    "-  _Spark Stages_ and _Spark Tasks_ are _Pipelined_ via map operations\n",
    "-  _Pipelining_ performs data dependent operations under the hood by collapsing them into single stage of tasks\n",
    "\n",
    "#### Shuffle Persistence:\n",
    "-  occurs when operations have to move data across nodes\n",
    "-  all shuffle operations will write data to disk for stable storage to use across multiple jobs\n",
    "\n",
    "\n",
    "#### SPARK.SQL.SHUFFLE.PARTITIONS:\n",
    "-  rule of thumb:\n",
    "    -  number of partitions > number of executors in cluster\n",
    "        -  cluster mode:\n",
    "            -  recommended to set according to the number of cores in cluster to ensure efficient execution\n",
    "        -  local mode:\n",
    "            -  recommended to set to low value since single machine cannot execute many tasks in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #15 Exercises (Spark Application)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Spark Submit Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n./bin/spark-submit     --class <main-class>     --master <master-url>     --deploy-mode cluster     --conf <key>=<value>     # other options\\n    <application.jar>     [application-arguments]\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "./bin/spark-submit \\\n",
    "    --class <main-class> \\\n",
    "    --master <master-url> \\\n",
    "    --deploy-mode cluster \\\n",
    "    --conf <key>=<value> \\\n",
    "    # other options\n",
    "    <application.jar> \\\n",
    "    [application-arguments]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _SparkSession (Manual Approach) Application Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder.master(\"local\").appName(\"Word Count).config(\"spark.some.config.option\", \"some-value\").getOrCreate()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    ".builder\\\n",
    ".master(\"local\")\\\n",
    ".appName(\"Word Count)\\\n",
    ".config(\"spark.some.config.option\", \"some-value\")\\\n",
    ".getOrCreate()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _SparkContext (Old Method) Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom pyspark import SparkContext\\n\\nsc = SparkContext.getOrCreate()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Spark Code Internal Execution Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sum(id)=2500000000000)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = spark.range(2, 10000000, 2)\n",
    "df2 = spark.range(2, 10000000, 4)\n",
    "step1 = df1.repartition(5)\n",
    "step12 = df2.repartition(6)\n",
    "step2 = step1.selectExpr(\"id * 5 as id\")\n",
    "step3 = step2.join(step12, [\"id\"])\n",
    "step4 = step3.selectExpr(\"sum(id)\")\n",
    "\n",
    "step4.collect() # 2500000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 1 w/ 8 Tasks [created DF; range function by default has 8 partitions]\n",
    "#### Stage 2 w/ 8 Tasks [created DF; range function by default has 8 partitions]\n",
    "#### Stage 3 w/ 6 Tasks [changed # of partitions to 6 by shuffling data]\n",
    "#### Stage 4 w/ 5 Tasks [changed # of partitions to 5 by shuffling data]\n",
    "#### Stage 5 w/ 200 Tasks [computed shuffle join with default 200 partitions; spark.sql.shuffle.partitions]\n",
    "#### Stage 6 w/ 1 Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(7) HashAggregate(keys=[], functions=[sum(id#6L)])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(6) HashAggregate(keys=[], functions=[partial_sum(id#6L)])\n",
      "      +- *(6) Project [id#6L]\n",
      "         +- *(6) SortMergeJoin [id#6L], [id#2L], Inner\n",
      "            :- *(3) Sort [id#6L ASC NULLS FIRST], false, 0\n",
      "            :  +- Exchange hashpartitioning(id#6L, 200)\n",
      "            :     +- *(2) Project [(id#0L * 5) AS id#6L]\n",
      "            :        +- Exchange RoundRobinPartitioning(5)\n",
      "            :           +- *(1) Range (2, 10000000, step=2, splits=8)\n",
      "            +- *(5) Sort [id#2L ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(id#2L, 200)\n",
      "                  +- Exchange RoundRobinPartitioning(6)\n",
      "                     +- *(4) Range (2, 10000000, step=4, splits=8)\n"
     ]
    }
   ],
   "source": [
    "step4.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #16 - Developing Spark Applications_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Spark Applications consist of:\n",
    "    -  Spark Cluster\n",
    "    -  code\n",
    "-  Spark Scala Applications:\n",
    "    -  build applications via JVM-based build tools:\n",
    "        -  sbt:\n",
    "            -  specify \"build.sbt\" containing package information:\n",
    "                -  project metadata (package name, version, information, etc.)\n",
    "                -  where to resolve dependencies\n",
    "                -  library dependencies\n",
    "            -  either use \"sbt assemble\" to build .JAR containing all dependencies in 1 .JAR or ...\n",
    "            -  use \"sbt package\" to gather all dependencies into target folder (won't package all in 1 .JAR)\n",
    "        -  Apache Maven\n",
    "-  Spark Python Applications:\n",
    "    -  execute .py scripts since Spark doesn't have a build method for Python\n",
    "    -  possible to package multiple Python files into egg or ZIP files of Spark code via --py-files argument which:\n",
    "        -  adds .py, .zip, .egg files to be distributed with application\n",
    "-  Testing Spark Applications:\n",
    "    -  things to keep in mind ... :\n",
    "        -  input data resilience\n",
    "        -  business logic resilience\n",
    "        -  output data resilience\n",
    "-  Spark Unit Testing:\n",
    "    -  JUnit\n",
    "    -  ScalaTest\n",
    "-  Spark Development Process:\n",
    "    -  interactive applications (initial development) => shell\n",
    "    -  production applications (submit to cluster) => spark-submit\n",
    "-  Launching Spark Applications:\n",
    "    -  client mode or cluster mode:\n",
    "        -  cluster mode is recommended to reduce latency between executors and driver\n",
    "        -  however client mode node is sometimes apart of the cluster\n",
    "    -  _Table 16.1 => Spark Submit Help_\n",
    "    -  _Table 16.2 => Spark Deployment Configuration_\n",
    "-  Spark Configuration:\n",
    "    -  SparkConf:\n",
    "        -  manages all of the applications configurations\n",
    "        -  controls how the Spark Application runs and how Spark Cluster is configued\n",
    "-  Spark Application Properties:\n",
    "    -  set via:\n",
    "        -  spark-submit during launch\n",
    "        -  code within Spark Application\n",
    "    -  confirm parameters via Spark UI \"Environment\" tab\n",
    "    -  _Table 16.3 => Spark Application Properties_\n",
    "    -  _Runtime Properties => http://spark.apache.org/docs/latest/configuration.html#runtime-environment _\n",
    "    -  _Execution Properties => http://spark.apache.org/docs/latest/configuration.html#execution-behavior _\n",
    "    -  _Memory Properties => http://spark.apache.org/docs/latest/configuration.html#memory-management _\n",
    "    -  _Shuffle Properties => http://spark.apache.org/docs/latest/configuration.html#shuffle-behavior _\n",
    "    -  Environmental Variables:\n",
    "        -  JAVA_HOME =>  location where java is installed\n",
    "        -  PYSPARK_PYTHON => python binary executable location for pyspark in driver and workers\n",
    "        -  PYSPARK_DRIVER_PYTHON => python binary executable location for pyspark in driver only\n",
    "        -  SPARKR_DRIVER_R => r binary executable for sparkR\n",
    "        -  SPARK_LOCAL_IP => ip address of the machine to bind\n",
    "        -  SPARK_PUBLIC_DNS => hostname spark program will advertise to other machines\n",
    "-  Spark Application Job Scheduling:\n",
    "    -  runs in FIFO (first in first out) fashion\n",
    "    -  set _spark.scheduler.mode_ to FAIR to enable a fair scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #16 Exercises (Spark Application Development)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Spark Scala Application Build Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n// Package Information\\n\\nname := \"example\" // change to project name\\norganization := \"com.databricks\" // change to your org\\nversion := \"0.1-SNAPSHOT\"\\nscalaVersion := \"2.11.8\"\\n\\n// Spark Information\\nval sparkVersion = \"2.1.0\"\\n\\n// allows us to include spark packages\\nresolvers += \"bintray-spark-packages\" at\\n  \"https://dl.bintray.com/spark-packages/maven/\"\\n\\nresolvers += \"Typesafe Simple Repository\" at\\n  \"http://repo.typesafe.com/typesafe/simple/maven-releases/\"\\n\\nresolvers += \"MavenRepository\" at\\n  \"https://mvnrepository.com/\"\\n\\nlibraryDependencies ++= Seq(\\n  // spark core\\n  \"org.apache.spark\" %% \"spark-core\" % sparkVersion,\\n  \"org.apache.spark\" %% \"spark-sql\" % sparkVersion,\\n\\n  // spark-modules\\n  \"org.apache.spark\" %% \"spark-graphx\" % sparkVersion,\\n  // \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion,\\n\\n  // spark packages\\n  \"graphframes\" % \"graphframes\" % \"0.4.0-spark2.1-s_2.11\",\\n\\n  // testing\\n  \"org.scalatest\" %% \"scalatest\" % \"2.2.4\" % \"test\",\\n  \"org.scalacheck\" %% \"scalacheck\" % \"1.12.2\" % \"test\",\\n\\n  // logging\\n  \"org.apache.logging.log4j\" % \"log4j-api\" % \"2.4.1\",\\n  \"org.apache.logging.log4j\" % \"log4j-core\" % \"2.4.1\"\\n)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "// Package Information\n",
    "\n",
    "name := \"example\" // change to project name\n",
    "organization := \"com.databricks\" // change to your org\n",
    "version := \"0.1-SNAPSHOT\"\n",
    "scalaVersion := \"2.11.8\"\n",
    "\n",
    "// Spark Information\n",
    "val sparkVersion = \"2.1.0\"\n",
    "\n",
    "// allows us to include spark packages\n",
    "resolvers += \"bintray-spark-packages\" at\n",
    "  \"https://dl.bintray.com/spark-packages/maven/\"\n",
    "\n",
    "resolvers += \"Typesafe Simple Repository\" at\n",
    "  \"http://repo.typesafe.com/typesafe/simple/maven-releases/\"\n",
    "\n",
    "resolvers += \"MavenRepository\" at\n",
    "  \"https://mvnrepository.com/\"\n",
    "\n",
    "libraryDependencies ++= Seq(\n",
    "  // spark core\n",
    "  \"org.apache.spark\" %% \"spark-core\" % sparkVersion,\n",
    "  \"org.apache.spark\" %% \"spark-sql\" % sparkVersion,\n",
    "\n",
    "  // spark-modules\n",
    "  \"org.apache.spark\" %% \"spark-graphx\" % sparkVersion,\n",
    "  // \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion,\n",
    "\n",
    "  // spark packages\n",
    "  \"graphframes\" % \"graphframes\" % \"0.4.0-spark2.1-s_2.11\",\n",
    "\n",
    "  // testing\n",
    "  \"org.scalatest\" %% \"scalatest\" % \"2.2.4\" % \"test\",\n",
    "  \"org.scalacheck\" %% \"scalacheck\" % \"1.12.2\" % \"test\",\n",
    "\n",
    "  // logging\n",
    "  \"org.apache.logging.log4j\" % \"log4j-api\" % \"2.4.1\",\n",
    "  \"org.apache.logging.log4j\" % \"log4j-core\" % \"2.4.1\"\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Spark Scala Application Directory Structure Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsrc/\\n    main/\\n        resources/\\n            <files to include in main jar here>\\n    scala/\\n        <main Scala sources>\\n    java/\\n        <main Java sources>\\n    test/\\n        resources\\n            <files to include in test jar here>\\n        scala/\\n            <test Scala sources>\\n        java/\\n            <test Java sources>\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "src/\n",
    "    main/\n",
    "        resources/\n",
    "            <files to include in main jar here>\n",
    "    scala/\n",
    "        <main Scala sources>\n",
    "    java/\n",
    "        <main Java sources>\n",
    "    test/\n",
    "        resources\n",
    "            <files to include in test jar here>\n",
    "        scala/\n",
    "            <test Scala sources>\n",
    "        java/\n",
    "            <test Java sources>\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Spark Scala Application SparkSession Initializer Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nobject DataFrameExample extends Serializable {\\n    def main(args: Array[String]) = {\\n\\n    val pathToDataFolder = args(0)\\n\\n// start up the SparkSession\\n// along with explicitly setting a given config\\nval spark = SparkSession.builder().appName(\"Spark Example\")\\n.config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\\n.getOrCreate()\\n\\n// udf registration\\nspark.udf.register(\"myUDF\", someUDF(_:String):String)\\nval df = spark.read.json(pathToDataFolder + \"data.json\")\\nval manipulated = df.groupBy(expr(\"myUDF(group)\")).sum().collect()\\n.foreach(x => println(x))\\n\\n    }\\n}\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "object DataFrameExample extends Serializable {\n",
    "    def main(args: Array[String]) = {\n",
    "\n",
    "    val pathToDataFolder = args(0)\n",
    "\n",
    "// start up the SparkSession\n",
    "// along with explicitly setting a given config\n",
    "val spark = SparkSession.builder().appName(\"Spark Example\")\n",
    ".config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\n",
    ".getOrCreate()\n",
    "\n",
    "// udf registration\n",
    "spark.udf.register(\"myUDF\", someUDF(_:String):String)\n",
    "val df = spark.read.json(pathToDataFolder + \"data.json\")\n",
    "val manipulated = df.groupBy(expr(\"myUDF(group)\")).sum().collect()\n",
    ".foreach(x => println(x))\n",
    "\n",
    "    }\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Running Spark Scala Application Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n$SPARK_HOME/bin/spark-submit --class com.databricks.example.DataFrameExample --master local target/scala-2.11/example_2.11-0.1-SNAPSHOT.jar \"hello\"\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "$SPARK_HOME/bin/spark-submit \\\n",
    "--class com.databricks.example.DataFrameExample \\\n",
    "--master local \\\n",
    "target/scala-2.11/example_2.11-0.1-SNAPSHOT.jar \"hello\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Spark Python Application Execution Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# in Python\\nfrom __future__ import print_function\\nif __name__ == \\'__main__\\':\\n    from pyspark.sql import SparkSession\\n    spark = SparkSession.builder         .master(\"local\")         .appName(\"Word Count\")         .config(\"spark.some.config.option\", \"some-value\")         .getOrCreate()\\n        \\n    print(spark.range(5000).where(\"id > 500\").selectExpr(\"sum(id)\").collect())\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# in Python\n",
    "from __future__ import print_function\n",
    "if __name__ == '__main__':\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Word Count\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "    print(spark.range(5000).where(\"id > 500\").selectExpr(\"sum(id)\").collect())\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Running Spark Python Application Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n$SPARK_HOME/bin/spark-submit --master local pyspark_template/main.py\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "$SPARK_HOME/bin/spark-submit --master local pyspark_template/main.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Spark Submit Help Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n> spark-submit --help\\nUsage: spark-submit [options] <app jar | python file | R file> [app arguments]\\nUsage: spark-submit --kill [submission ID] --master [spark://...]\\nUsage: spark-submit --status [submission ID] --master [spark://...]\\nUsage: spark-submit run-example [options] example-class [example args]\\n\\nOptions:\\n  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\\n                              k8s://https://host:port, or local (Default: local[*]).\\n  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\\n                              on one of the worker machines inside the cluster (\"cluster\")\\n                              (Default: client).\\n  --class CLASS_NAME          Your application\\'s main class (for Java / Scala apps).\\n  --name NAME                 A name of your application.\\n  --jars JARS                 Comma-separated list of jars to include on the driver\\n                              and executor classpaths.\\n  --packages                  Comma-separated list of maven coordinates of jars to include\\n                              on the driver and executor classpaths. Will search the local\\n                              maven repo, then maven central and any additional remote\\n                              repositories given by --repositories. The format for the\\n                              coordinates should be groupId:artifactId:version.\\n  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\\n                              resolving the dependencies provided in --packages to avoid\\n                              dependency conflicts.\\n  --repositories              Comma-separated list of additional remote repositories to\\n                              search for the maven coordinates given with --packages.\\n  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\\n                              on the PYTHONPATH for Python apps.\\n  --files FILES               Comma-separated list of files to be placed in the working\\n                              directory of each executor. File paths of these files\\n                              in executors can be accessed via SparkFiles.get(fileName).\\n\\n  --conf PROP=VALUE           Arbitrary Spark configuration property.\\n  --properties-file FILE      Path to a file from which to load extra properties. If not\\n                              specified, this will look for conf/spark-defaults.conf.\\n\\n  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\\n  --driver-java-options       Extra Java options to pass to the driver.\\n  --driver-library-path       Extra library path entries to pass to the driver.\\n  --driver-class-path         Extra class path entries to pass to the driver. Note that\\n                              jars added with --jars are automatically included in the\\n                              classpath.\\n\\n  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\\n\\n  --proxy-user NAME           User to impersonate when submitting the application.\\n                              This argument does not work with --principal / --keytab.\\n\\n  --help, -h                  Show this help message and exit.\\n  --verbose, -v               Print additional debug output.\\n  --version,                  Print the version of current Spark.\\n\\n Cluster deploy mode only:\\n  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\\n                              (Default: 1).\\n\\n Spark standalone or Mesos with cluster deploy mode only:\\n  --supervise                 If given, restarts the driver on failure.\\n  --kill SUBMISSION_ID        If given, kills the driver specified.\\n  --status SUBMISSION_ID      If given, requests the status of the driver specified.\\n\\n Spark standalone and Mesos only:\\n  --total-executor-cores NUM  Total cores for all executors.\\n\\n Spark standalone and YARN only:\\n  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,\\n                              or all available cores on the worker in standalone mode)\\n\\n YARN-only:\\n  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\\n  --num-executors NUM         Number of executors to launch (Default: 2).\\n                              If dynamic allocation is enabled, the initial number of\\n                              executors will be at least NUM.\\n  --archives ARCHIVES         Comma separated list of archives to be extracted into the\\n                              working directory of each executor.\\n  --principal PRINCIPAL       Principal to be used to login to KDC, while running on\\n                              secure HDFS.\\n  --keytab KEYTAB             The full path to the file that contains the keytab for the\\n                              principal specified above. This keytab will be copied to\\n                              the node running the Application Master via the Secure\\n                              Distributed Cache, for renewing the login tickets and the\\n                              delegation tokens periodically.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "> spark-submit --help\n",
    "Usage: spark-submit [options] <app jar | python file | R file> [app arguments]\n",
    "Usage: spark-submit --kill [submission ID] --master [spark://...]\n",
    "Usage: spark-submit --status [submission ID] --master [spark://...]\n",
    "Usage: spark-submit run-example [options] example-class [example args]\n",
    "\n",
    "Options:\n",
    "  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n",
    "                              k8s://https://host:port, or local (Default: local[*]).\n",
    "  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n",
    "                              on one of the worker machines inside the cluster (\"cluster\")\n",
    "                              (Default: client).\n",
    "  --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n",
    "  --name NAME                 A name of your application.\n",
    "  --jars JARS                 Comma-separated list of jars to include on the driver\n",
    "                              and executor classpaths.\n",
    "  --packages                  Comma-separated list of maven coordinates of jars to include\n",
    "                              on the driver and executor classpaths. Will search the local\n",
    "                              maven repo, then maven central and any additional remote\n",
    "                              repositories given by --repositories. The format for the\n",
    "                              coordinates should be groupId:artifactId:version.\n",
    "  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n",
    "                              resolving the dependencies provided in --packages to avoid\n",
    "                              dependency conflicts.\n",
    "  --repositories              Comma-separated list of additional remote repositories to\n",
    "                              search for the maven coordinates given with --packages.\n",
    "  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n",
    "                              on the PYTHONPATH for Python apps.\n",
    "  --files FILES               Comma-separated list of files to be placed in the working\n",
    "                              directory of each executor. File paths of these files\n",
    "                              in executors can be accessed via SparkFiles.get(fileName).\n",
    "\n",
    "  --conf PROP=VALUE           Arbitrary Spark configuration property.\n",
    "  --properties-file FILE      Path to a file from which to load extra properties. If not\n",
    "                              specified, this will look for conf/spark-defaults.conf.\n",
    "\n",
    "  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
    "  --driver-java-options       Extra Java options to pass to the driver.\n",
    "  --driver-library-path       Extra library path entries to pass to the driver.\n",
    "  --driver-class-path         Extra class path entries to pass to the driver. Note that\n",
    "                              jars added with --jars are automatically included in the\n",
    "                              classpath.\n",
    "\n",
    "  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
    "\n",
    "  --proxy-user NAME           User to impersonate when submitting the application.\n",
    "                              This argument does not work with --principal / --keytab.\n",
    "\n",
    "  --help, -h                  Show this help message and exit.\n",
    "  --verbose, -v               Print additional debug output.\n",
    "  --version,                  Print the version of current Spark.\n",
    "\n",
    " Cluster deploy mode only:\n",
    "  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n",
    "                              (Default: 1).\n",
    "\n",
    " Spark standalone or Mesos with cluster deploy mode only:\n",
    "  --supervise                 If given, restarts the driver on failure.\n",
    "  --kill SUBMISSION_ID        If given, kills the driver specified.\n",
    "  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n",
    "\n",
    " Spark standalone and Mesos only:\n",
    "  --total-executor-cores NUM  Total cores for all executors.\n",
    "\n",
    " Spark standalone and YARN only:\n",
    "  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,\n",
    "                              or all available cores on the worker in standalone mode)\n",
    "\n",
    " YARN-only:\n",
    "  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
    "  --num-executors NUM         Number of executors to launch (Default: 2).\n",
    "                              If dynamic allocation is enabled, the initial number of\n",
    "                              executors will be at least NUM.\n",
    "  --archives ARCHIVES         Comma separated list of archives to be extracted into the\n",
    "                              working directory of each executor.\n",
    "  --principal PRINCIPAL       Principal to be used to login to KDC, while running on\n",
    "                              secure HDFS.\n",
    "  --keytab KEYTAB             The full path to the file that contains the keytab for the\n",
    "                              principal specified above. This keytab will be copied to\n",
    "                              the node running the Application Master via the Secure\n",
    "                              Distributed Cache, for renewing the login tickets and the\n",
    "                              delegation tokens periodically.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _SparkConf Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom pyspark import SparkConf\\nconf = SparkConf().setMaster(\"local[2]\").setAppName(\"DefinitiveGuide\")  .set(\"some.conf\", \"to.some.value\")\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from pyspark import SparkConf\n",
    "conf = SparkConf().setMaster(\"local[2]\").setAppName(\"DefinitiveGuide\")\\\n",
    "  .set(\"some.conf\", \"to.some.value\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #17 - Deploying Spark_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Managers:\n",
    "-  cluster manager documentation => http://spark.apache.org/docs/latest/cluster-overview.html\n",
    "-  manages set of machines to deploy Spark Applications including:\n",
    "    -  Standalone:\n",
    "        -  built specifically for Spark workloads\n",
    "        -  only runs Spark framework\n",
    "        -  run multiple Spark Applications on the same cluster\n",
    "        -  environment variables documentation => http://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts\n",
    "        -  set --master to 'master node IP'\n",
    "    -  YARN:\n",
    "        -  set --master to 'yarn'\n",
    "        -  deployment modes:\n",
    "            -  cluster mode:\n",
    "                -  Spark Driver process managed by YARN\n",
    "                -  client can exit after creating application\n",
    "                -  YARN picks a machine (may not be machine used to manaully execute application) as Master\n",
    "            -  client mode:\n",
    "                -  Spark Driver process runs in client process\n",
    "                -  YARN is only responsible for granting executor resources to application\n",
    "                -  YARN does not maintain Spark Driver\n",
    "        -  enable HDFS read/write for Spark:\n",
    "            -  set HADOOP_CONF_DIR in SPARK_HOME/spark-env.sh to location containing hdfs-site.xml & core-site.xml\n",
    "        -  YARN configurations documentation => http://spark.apache.org/docs/latest/running-on-yarn.html#configuration\n",
    "    -  Mesos:\n",
    "        -  abstracts cpu, memory, storage and other resources from machines (physical and virtual)\n",
    "        -  uses coarse-grain mode:\n",
    "            -  each Spark executor runs as a single Mesos task\n",
    "        -  supports client and cluster mode\n",
    "        -  Mesos configurations documentation => http://spark.apache.org/docs/latest/running-on-mesos.html#configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Cluster Deployment Options:\n",
    "-  On Premise Cluster:\n",
    "    -  pros:\n",
    "        -  secure private datacenters\n",
    "        -  full control over hardware to optimize workloads\n",
    "    -  cons:\n",
    "        -  fixed cluster size\n",
    "        -  resource sharing\n",
    "        -  elastic resource demands (OP cluster may not have enough horsepower to support ML/data analytics)\n",
    "        -  operate own storage system (ex: HDFS [distributed file system]; Cassandra [key-value store])\n",
    "        -  setup of georeplication and disaster recovery\n",
    "-  Public Cloud Cluster:\n",
    "    -  pros:\n",
    "        -  provide applications its own cluster\n",
    "        -  customize cluster size per job to optimize cost performance\n",
    "        -  launch/shut down resources elastically\n",
    "        -  utilize GPUs for DL jobs\n",
    "        -  elastic low cost storage (ex: AWS; Azure; GCP)\n",
    "    -  cons:\n",
    "        -  fixed cluster size and file system lacks elasticity (ex: EMR)\n",
    "    -  alternative:\n",
    "        -  recommended to use global storage systems (ex: S3, Azure Blob, GCS) decoupled from specific cluster\n",
    "        -  **decouple compute and storage to spin up machines dynamically for each Spark workload**\n",
    "-  Secure Deployment Configuration:\n",
    "    -  security configurations documentation => http://spark.apache.org/docs/latest/configuration.html#security\n",
    "-  Cluster Networking Configuration:\n",
    "    -  networking configurations documentation => https://spark.apache.org/docs/latest/configuration.html#networking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Application Scheduling:\n",
    "-  each Spark Application runs an independent set of executor processes\n",
    "-  Spark has the ability via configuration to use a Fair Scheduler to schedule resources within each application\n",
    "-  use Static Partitioning of resources if multiple users share your cluster and run different Spark Applications:\n",
    "    -  static partitioning allocates a maximum amount of resources each application can utilize\n",
    "-  job scheduling configurations documentation => https://spark.apache.org/docs/latest/configuration.html#scheduling\n",
    "\n",
    "### Spark Dynamic Allocation:\n",
    "-  allows applications to scale resources up and down dynamically based on workload needs\n",
    "-  disabled by default except in some vendor distributions (Cloudera, Hortonworks)\n",
    "-  configure via parameters:\n",
    "    -  _spark.dynamicAllocation.enabled_ to TRUE\n",
    "    -  _spark.shuffle.service.enabled_ to TRUE\n",
    "-  dynamic allocation configurations documentation => https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #17 Exercises (Spark Application Deployment)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Spark Standalone Deployment By Hand Example_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how to start cluster by hand? :\n",
    "    #1 - start cluster manager master process => $SPARK_HOME/sbin/start-master.sh\n",
    "    #2 - master prints URI => spark://HOST:PORT\n",
    "    #3 - log into each worker machine & start node w/ URI => $SPARK_HOME/sbin/start-slave.sh <master-spark-URI>\n",
    "    #4 - submit applications via spark-submit => spark://URI of master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Spark Standalone Deployment By Automated Scripts Example_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how to start cluster by automated scripts? :\n",
    "        #1 - create file called conf/slaves in Spark directory containing all hostnames of machines\n",
    "        #2 - launch and stop cluster via shell scripts available in $SPARK_HOME/sbin\n",
    "\n",
    "# $SPARK_HOME/sbin/start-master.sh\n",
    "    # Starts a master instance on the machine on which the script is executed.\n",
    "# $SPARK_HOME/sbin/start-slaves.sh\n",
    "    # Starts a slave instance on each machine specified in the conf/slaves file.\n",
    "# $SPARK_HOME/sbin/start-slave.sh\n",
    "    # Starts a slave instance on the machine on which the script is executed.\n",
    "# $SPARK_HOME/sbin/start-all.sh\n",
    "    # Starts both a master and a number of slaves as described earlier.\n",
    "# $SPARK_HOME/sbin/stop-master.sh\n",
    "    # Stops the master that was started via the bin/start-master.sh script.\n",
    "# $SPARK_HOME/sbin/stop-slaves.sh\n",
    "    # Stops all slave instances on the machines specified in the conf/slaves file.\n",
    "# $SPARK_HOME/sbin/stop-all.sh\n",
    "    # Stops both the master and the slaves as described earlier.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #18 - Monitoring and Debugging_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Monitoring Landscape:\n",
    "-  Spark Application & Jobs:\n",
    "    -  Spark UI\n",
    "    -  Spark Logs\n",
    "-  JVM:\n",
    "    -  where Spark Executors run\n",
    "    -  utilities:\n",
    "        -  jstack => provides stack traces\n",
    "        -  jmap => creates heap-dumps\n",
    "        -  jstat => reports time-series stats\n",
    "        -  jconsole => visually explores many JVM properties\n",
    "        -  jvisualvm => can be used to help profile Spark jobs\n",
    "-  OS/Machine:\n",
    "    -  host OS where JVMs run\n",
    "    -  monitor health of CPU, network, I/O\n",
    "    -  available tools like dstat, iostat, iotop\n",
    "-  Cluster:\n",
    "    -  depends on CM\n",
    "    -  things like YARN UI, Ganglia, and Prometheus are options\n",
    "\n",
    "### Spark Monitoring Components:\n",
    "-  Processes running application (CPU usage, memory usage, etc.):\n",
    "    -  keep an eye on driver [**state where application lives**]\n",
    "    -  also monitor state of executors\n",
    "-  Query Execution inside process (jobs / stages / tasks):\n",
    "    -  Spark Logs\n",
    "    -  Spark UI:\n",
    "        -  ui configurations documentation => https://spark.apache.org/docs/latest/configuration.html#spark-ui; https://spark.apache.org/docs/latest/monitoring.html#spark-configuration-options\n",
    "        -  tabs:\n",
    "            -  Jobs => shows Spark Jobs\n",
    "            -  Stages => shows Spark Stages and their associated Tasks\n",
    "            -  Storage => shows cached data in Spark Application\n",
    "            -  Environment => shows configuration information and current settings of Spark Application\n",
    "            -  Executors => shows information about each Spark Executor running Spark Application\n",
    "            -  SQL => shows Structured API queries (SQL / DFs)\n",
    "    -  Spark REST API:\n",
    "        -  another method to access Spark's status and metrics => http://localhost:4040/api/v1\n",
    "        -  useful for custom built reporting solutions\n",
    "        -  rest api monitoring documentation => https://spark.apache.org/docs/latest/monitoring.html#rest-api\n",
    "    -  Spark History Server:\n",
    "        -  stores historical Spark logs\n",
    "        -  configure application to store event logs to a certain location:\n",
    "            -  _spark.eventLog.enabled_\n",
    "            -  _spark.eventLog.dir_\n",
    "        -  runs as standalone applications\n",
    "        -  history server configurations documentation => https://spark.apache.org/docs/latest/monitoring.html#spark-configuration-options "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Debugging - Common Problems:\n",
    "1.  Spark Jobs Not Starting:\n",
    "    -  resources requested not available\n",
    "    -  cluster misconfiguration\n",
    "2.  Errors Before Execution:\n",
    "    -  typo / incorrect column name\n",
    "    -  network disconnection\n",
    "3.  Errors During Execution:\n",
    "    -  bad input data:\n",
    "        -  null values\n",
    "        -  incorrect schema\n",
    "        -  row of data doesn't match schema\n",
    "4.  Slow Tasks or Stragglers:\n",
    "    -  data is partitioned unevenly across cluster => increase # of partitions to have less data per partition\n",
    "    -  not enough memory allocated to executors => allocate more memory\n",
    "    -  machine may have a hardware problem / disk full\n",
    "5.  Slow Aggregations:\n",
    "    -  data skews with keys being grouped => increase # of partitions prior to aggregation\n",
    "    -  \"empty\" values => change to _null_\n",
    "6.  Slow Joins:\n",
    "     -  \"empty\" values => change to _null_\n",
    "     -  inefficient join type => optimize with another join type / filter data first then adjust join order\n",
    "     -  data skews => partition prior to joining / increase executor memory\n",
    "7.  Slow Reads and Writes:\n",
    "    -  bad network connectivity\n",
    "8.  Driver OutOfMemoryError or Driver Unresponsive / GC Messages:\n",
    "    -  too much data being collected back to driver (runs out of memory) => increase driver memory allocation\n",
    "9.  Executor OutOfMemoryError or Executor Unresponsive / GC Messages:\n",
    "    -  executors crash => increase executor memory and # of executors\n",
    "    -  python memory problem => increase PySpark worker size\n",
    "    -  garbage collection errors => repartition data to increase parallelism to reduce amount of records per task and ensure executors are getting same amount of work to process\n",
    "    -  \"empty\" values => change to _null_\n",
    "    -  avoid using UDFs if possible\n",
    "10.  Unexpected Nulls in Results:\n",
    "    -  volatile data\n",
    "11.  No Space Level on Disk Errors:\n",
    "    -  not enough space => add more disk space\n",
    "12.  Serialization Errors:\n",
    "    -  data cannot be serialized => usually via RDDs or UDFs\n",
    "    -  default serialization => change to Kryo serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #18 Exercises (Spark Application Monitoring)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(is_glass=None, count=1454),\n",
       " Row(is_glass=True, count=12861),\n",
       " Row(is_glass=False, count=527594)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(online)\\\n",
    ".repartition(2)\\\n",
    ".selectExpr(\"instr(Description, 'GLASS') >= 1 as is_glass\")\\\n",
    ".groupBy(\"is_glass\")\\\n",
    ".count()\\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Spark UI metrics:\\n\\n// SQL TAB:\\n    // Summary Statistics => metrics for query\\n    // DAG of Spark Stages => each BLUE box represents a Spark Stage of Spark Tasks\\n    // Spark Job => entire group of Spark Stages represent a Spark Job\\n\\n// JOBS TAB:\\n    // shows Spark Jobs\\n    // shows Spark Stages within Job ID\\n    // shows Spark Tasks within Stage ID\\n    // Summary Metrics => monitoring statistics (be on the lookout for outliers / distribution of values)\\n    // Aggregated Metrics by Executor => examine Spark Executor performance\\n    // Show Additional Metrics => provides more advanced metrics\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Spark UI metrics:\n",
    "\n",
    "// SQL TAB:\n",
    "    // Summary Statistics => metrics for query\n",
    "    // DAG of Spark Stages => each BLUE box represents a Spark Stage of Spark Tasks\n",
    "    // Spark Job => entire group of Spark Stages represent a Spark Job\n",
    "\n",
    "// JOBS TAB:\n",
    "    // shows Spark Jobs\n",
    "    // shows Spark Stages within Job ID\n",
    "    // shows Spark Tasks within Stage ID\n",
    "    // Summary Metrics => monitoring statistics (be on the lookout for outliers / distribution of values)\n",
    "    // Aggregated Metrics by Executor => examine Spark Executor performance\n",
    "    // Show Additional Metrics => provides more advanced metrics\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Spark Log Level Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# allows log reading\n",
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Chapter #19 - Performance Tuning_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussed Topics:\n",
    "-  DFs vs RDDs:\n",
    "    -  recommended to use Scala/Java for RDDs\n",
    "    -  Python expensively serializes data to and from Python process when running RDD code\n",
    "-  RDD Object Serialization:\n",
    "    -  set _spark.serializer_ to _org.apache.spark.serializer.KryoSerializer_\n",
    "-  Dynamic Allocation:\n",
    "    -  dynamically allocate resources\n",
    "    -  set _spark.dynamicAllocation.enabled_ to _true_\n",
    "-  Scheduling:\n",
    "    -  set _spark.scheduler.mode_ to _FAIR_\n",
    "    -  FAIR provides better sharing of resources across multiple users\n",
    "    -  set --max-executor-cores to specify max # of executor cores application will need\n",
    "    -  --max-executor-cores ensures application won't consume up all the resources on cluster\n",
    "-  Data Storage Format:\n",
    "    -  favor structured binary type for frequent access\n",
    "    -  Apache Parquet is best\n",
    "-  Splittable File Types / Compression:\n",
    "    -  \"splittable\" => different tasks can read different parts of the file in parallel\n",
    "    -  use compression like gzip, snappy\n",
    "    -  **try to keep individual read files no larger than a few hundred MB for \"splittable\" purpose**\n",
    "-  Table Partitioning:\n",
    "    -  stores files in separate directories based on a key (column)\n",
    "    -  improves speed, filtering, and spread of data across cluster\n",
    "-  Bucketing:\n",
    "    -  allows Spark to \"pre-partition\" data according to how joins or aggregations are performed\n",
    "    -  helps with partitioning / prevent shuffle before join / data access speed\n",
    "-  Number of Files:\n",
    "    -  avoid many small files and many large files\n",
    "    -  **rule of thumb is to aim for each written file to be around a few tens of MB**\n",
    "    -  _maxRecordsPerFile_ => controls how many records go into each file\n",
    "-  Data Locality:\n",
    "    -  specifies a preference for certain nodes that hold certain data\n",
    "    -  avoids exchanging blocks of data over the network\n",
    "    -  local storage (ex: HDFS) marked as \"local\" in Spark UI tasks\n",
    "-  Table Statistics:\n",
    "    -  table level stats => ANALYZE TABLE tableName COMPUTE STATISTICS\n",
    "    -  column level stats => ANALYZE TABLE tableName COMPUTE STATISTICS FOR COLUMNS column1, column2, etc.\n",
    "-  Shuffle Configurations (Spark's External Shuffle Servce):\n",
    "    -  helps increase performance because nodes read shuffle data from remote machines even when executors on machines are busy\n",
    "    -  small partitions lead to some nodes being under-utilized and data skews\n",
    "    -  large partitions lead to overhead and some nodes dominating\n",
    "    -  **aim for around a few tens of MB of data per output partition in shuffle**\n",
    "-  OOM / GC:\n",
    "    -  gather statistics via _spark.executor.extraJavaOptions_ => -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps\n",
    "    -  logs located on cluster's worker nodes (in stdout files)\n",
    "-  Parallelism:\n",
    "    -  increase parallelism to speed up stage\n",
    "    -  recommendations:\n",
    "        -  use 2 or 3 tasks per CPU cores in cluster if stage processes large amount of data:\n",
    "            -  set _spark.default.parallelism_\n",
    "            -  set _spark.sql.shuffle.partitions_ => number of cores in cluster\n",
    "-  Filtering:\n",
    "    -  always filter has early on as possible\n",
    "-  Repartitioning / Coalescing:\n",
    "    -  repartition:\n",
    "        -  incurs a shuffle\n",
    "        -  optimizes execution/parallelism by load balancing data across cluster\n",
    "        -  helpful for joins/cached data\n",
    "    -  coalesce:\n",
    "        -  reduces # of shuffles by merging partitions on nodes\n",
    "-  UDFS:\n",
    "    -  expensive operations\n",
    "    -  force representing data as objects in JVM\n",
    "-  Temporary Data Storage (Caching):\n",
    "    -  _Table 19.1 => Data Cache Storage Levels_\n",
    "    -  reuse same dataset over and over\n",
    "    -  places DF, table, RDD into temporary storage (memory or disk) across executors in cluster\n",
    "    -  helps with faster reads\n",
    "    -  negative impacts => incurs serialization, deserialization, storage costs\n",
    "    -  lazy operation / only cached when used (action)\n",
    "    -  default cache is in memory\n",
    "    -  RDD Cache:\n",
    "        -  physical data (bits) is cached as object\n",
    "    -  DF Cache:\n",
    "        -  physical plan is cached => physical plan is stored as key and performs lookup prior to execution of Structured job\n",
    "    -  Storage Levels:\n",
    "        -  MEMORY_ONLY (default)\n",
    "        -  MEMORY_AND_DISK\n",
    "        -  MEMORY_ONLY_SER\n",
    "        -  MEMORY_AND_DISK_SER\n",
    "        -  DISK_ONLY\n",
    "        -  MEMORY_ONLY_2\n",
    "        -  MEMORY_AND_DISK_2\n",
    "        -  OFF_HEAP\n",
    "        -  https://spark.apache.org/docs/latest/rdd-programming-guide.html#which-storage-level-to-choose\n",
    "-  Joins:\n",
    "    -  equi joins are best when possible\n",
    "    -  cartesian/full joins should be avoided\n",
    "    -  bucketing helps with avoiding shuffles prior to joins\n",
    "-  Aggregations:\n",
    "    -  via RDDs use reduceByKey when possible over groupByKey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Spark Performance Advice In A Nutshell To Prioritize:\n",
    "1.  read as little data as possible through partitioning and efficient binary formats\n",
    "2.  sufficient parallelism and no data skews on cluster using partitioning\n",
    "3.  use high-level Stuctured APIs for optimized code\n",
    "4.  utilize monitoring tools (ex: Spark UI) to efficiently and effectively troubleshoot and optimize Spark Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Chapter #19 Exercises (Spark Application Tuning)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Garbage Collection Tuning Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmemory management in the JVM:\\nJava heap space is divided into two regions: Young and Old. The Young generation is meant\\nto hold short-lived objects whereas the Old generation is intended for objects with longer\\nlifetimes.\\n\\nThe Young generation is further divided into three regions: Eden, Survivor1, and Survivor2.\\n\\ngarbage collection procedure:\\n1. When Eden is full, a minor garbage collection is run on Eden and objects that are alive from\\nEden and Survivor1 are copied to Survivor2.\\n2. The Survivor regions are swapped.\\n3. If an object is old enough or if Survivor2 is full, that object is moved to Old.\\n4. Finally, when Old is close to full, a full garbage collection is invoked. This involves tracing\\nthrough all the objects on the heap, deleting the unreferenced ones, and moving the others to\\nfill up unused space, so it is generally the slowest garbage collection operation.\\n\\nThe goal of garbage collection tuning in Spark is to ensure that only long-lived cached datasets are\\nstored in the Old generation and that the Young generation is sufficiently sized to store all short-lived\\nobjects.\\n\\nIf a full garbage collection is invoked multiple times before a task completes, it means that there isnt enough memory\\navailable for executing tasks, so you should decrease the amount of memory Spark uses for caching\\n(spark.memory.fraction).\\n\\nTry the G1GC garbage collector with -XX:+UseG1GC. It can improve performance in some situations\\nin which garbage collection is a bottleneck and you dont have a way to reduce it further by sizing the\\ngenerations. Note that with large executor heap sizes, it can be important to increase the G1 region\\nsize with -XX:G1HeapRegionSize.\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "memory management in the JVM:\n",
    "Java heap space is divided into two regions: Young and Old. The Young generation is meant\n",
    "to hold short-lived objects whereas the Old generation is intended for objects with longer\n",
    "lifetimes.\n",
    "\n",
    "The Young generation is further divided into three regions: Eden, Survivor1, and Survivor2.\n",
    "\n",
    "garbage collection procedure:\n",
    "1. When Eden is full, a minor garbage collection is run on Eden and objects that are alive from\n",
    "Eden and Survivor1 are copied to Survivor2.\n",
    "2. The Survivor regions are swapped.\n",
    "3. If an object is old enough or if Survivor2 is full, that object is moved to Old.\n",
    "4. Finally, when Old is close to full, a full garbage collection is invoked. This involves tracing\n",
    "through all the objects on the heap, deleting the unreferenced ones, and moving the others to\n",
    "fill up unused space, so it is generally the slowest garbage collection operation.\n",
    "\n",
    "The goal of garbage collection tuning in Spark is to ensure that only long-lived cached datasets are\n",
    "stored in the Old generation and that the Young generation is sufficiently sized to store all short-lived\n",
    "objects.\n",
    "\n",
    "If a full garbage collection is invoked multiple times before a task completes, it means that there isnt enough memory\n",
    "available for executing tasks, so you should decrease the amount of memory Spark uses for caching\n",
    "(spark.memory.fraction).\n",
    "\n",
    "Try the G1GC garbage collector with -XX:+UseG1GC. It can improve performance in some situations\n",
    "in which garbage collection is a bottleneck and you dont have a way to reduce it further by sizing the\n",
    "generations. Note that with large executor heap sizes, it can be important to increase the G1 region\n",
    "size with -XX:G1HeapRegionSize.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Data Cache Storage Levels Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMEMORY_ONLY:\\nStore RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions\\nwill not be cached and will be recomputed on the fly each time theyre needed. This is the default level.\\n\\nMEMORY_AND_DISK:\\nStore RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the\\npartitions that dont fit on disk, and read them from there when theyre needed.\\n\\nMEMORY_ONLY_SER (Java and Scala):\\nStore RDD as serialized Java objects (one byte array per partition). This is generally more spaceefficient\\nthan deserialized objects, especially when using a fast serializer, but more CPU-intensive to\\nread.\\n\\nMEMORY_AND_DISK_SER (Java and Scala):\\nSimilar to MEMORY_ONLY_SER, but spill partitions that dont fit in memory to disk instead of recomputing\\nthem on the fly each time theyre needed.\\n\\nDISK_ONLY:\\nStore the RDD partitions only on disk.\\n\\nMEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.:\\nSame as the previous levels, but replicate each partition on two cluster nodes.\\n\\nOFF_HEAP (experimental):\\nSimilar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to\\nbe enabled.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "MEMORY_ONLY:\n",
    "Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions\n",
    "will not be cached and will be recomputed on the fly each time theyre needed. This is the default level.\n",
    "\n",
    "MEMORY_AND_DISK:\n",
    "Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the\n",
    "partitions that dont fit on disk, and read them from there when theyre needed.\n",
    "\n",
    "MEMORY_ONLY_SER (Java and Scala):\n",
    "Store RDD as serialized Java objects (one byte array per partition). This is generally more spaceefficient\n",
    "than deserialized objects, especially when using a fast serializer, but more CPU-intensive to\n",
    "read.\n",
    "\n",
    "MEMORY_AND_DISK_SER (Java and Scala):\n",
    "Similar to MEMORY_ONLY_SER, but spill partitions that dont fit in memory to disk instead of recomputing\n",
    "them on the fly each time theyre needed.\n",
    "\n",
    "DISK_ONLY:\n",
    "Store the RDD partitions only on disk.\n",
    "\n",
    "MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.:\n",
    "Same as the previous levels, but replicate each partition on two cluster nodes.\n",
    "\n",
    "OFF_HEAP (experimental):\n",
    "Similar to MEMORY_ONLY_SER, but store the data in off-heap memory. This requires off-heap memory to\n",
    "be enabled.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Cache DF Example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original loading code that does *not* cache DataFrame\n",
    "DF1 = spark.read.format(\"csv\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".load(flights2105)\n",
    "\n",
    "DF2 = DF1.groupBy(\"DEST_COUNTRY_NAME\").count().collect() # refers to original file\n",
    "DF3 = DF1.groupBy(\"ORIGIN_COUNTRY_NAME\").count().collect() # refers to original file\n",
    "DF4 = DF1.groupBy(\"count\").count().collect() # refers to original file\n",
    "\n",
    "DF1.cache()\n",
    "DF1.count()\n",
    "\n",
    "DF2 = DF1.groupBy(\"DEST_COUNTRY_NAME\").count().collect() # refers to new cached data in memory\n",
    "DF3 = DF1.groupBy(\"ORIGIN_COUNTRY_NAME\").count().collect() # refers to new cached data in memory\n",
    "DF4 = DF1.groupBy(\"count\").count().collect() # refers to new cached data in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
